{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Intro to Text Analysis with Python\n",
    "### Roadmap\n",
    "1. Working with data in pandas dataframes\n",
    "2. Preparing unstructured text data for analysis (tokenizing, stemming, and stop words)\n",
    "3. Document term matrix\n",
    "4. Two topic modeling alorigthms (Non-negative Matrix Factorization & Latent Dirichlet Allocation)\n",
    "5. Predictive text classifer (Naive-Bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Read in data files into a pandas dataframe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Force certain columns to be string data type instead of mix type\n",
    "#Dataframes will perform better\n",
    "dtypes = {\n",
    "    'Description' : object,\n",
    "    'Summary*' : object,\n",
    "    'Memo 2' : object,\n",
    "    'Contact Organization' : object,\n",
    "    'Contact Department' : object,\n",
    "    'Contact Desk Location' : object,\n",
    "    'Contact E-mail' : object, \n",
    "    'Memo 1' : object, \n",
    "    'Resolution Categorization Tier 1' : object\n",
    "}\n",
    "\n",
    "#Data was separated into three files so use pandas concat function to union them together\n",
    "#Python3 and pandas is preferred when working with text data as it is more compatible with unicode\n",
    "df = pd.concat([\n",
    "    pd.read_csv(\"../tickets.csv\", dtype=dtypes, encoding=\"ISO-8859-1\"),\n",
    "    pd.read_csv(\"../ticketsarc.csv\", dtype=dtypes, encoding=\"ISO-8859-1\"),\n",
    "    pd.read_csv(\"../ticketsarc2.csv\", dtype=dtypes, encoding=\"ISO-8859-1\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51069, 79)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check data (rows, columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#For analysis we will only work with a subset of rows and columns\n",
    "#We create a separate dataframe to hold this data\n",
    "\n",
    "#Select rows based on a criteria, i.e. the \"where\" clause\n",
    "dftxt = df[df['Operational Categorization Tier 3'].isin(['Cat A','Cat B'])]\n",
    "\n",
    "#Select only 4 of the 79 columns, i.e. the \"select\" clause\n",
    "dftxt = dftxt[['Incident ID*+','Operational Categorization Tier 3','Description','Summary*']]\n",
    "\n",
    "#Replace all null with empty strings to avoid any null pointer errors downstream\n",
    "dftxt = dftxt.replace(np.nan, '', regex=True) \n",
    "\n",
    "#Concatenate the two text columns into one single column\n",
    "dftxt['text'] = dftxt['Summary*'].map(str) + u'. ' + dftxt['Description'].map(str)\n",
    "\n",
    "#Remove the original two text columns\n",
    "dftxt = dftxt.drop(['Description','Summary*'], axis=1)\n",
    "\n",
    "#Reset the row counter of the new dataframe to start at 0\n",
    "dftxt.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7628, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check data subset (rows, columns)\n",
    "dftxt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Incident ID*+</th>\n",
       "      <th>Operational Categorization Tier 3</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID000011111111</td>\n",
       "      <td>Cat A</td>\n",
       "      <td>ERS Supplier. Request a change to information ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID000011111112</td>\n",
       "      <td>Cat B</td>\n",
       "      <td>85555 Fisk, Wen - confirming address. 55555 Fa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Incident ID*+ Operational Categorization Tier 3  \\\n",
       "0  ID000011111111                     Cat A   \n",
       "1  ID000011111112                      Cat B   \n",
       "\n",
       "                                                text  \n",
       "0  ERS Supplier. Request a change to information ...  \n",
       "1  85555 Fisk, Wen - confirming address. 55555 Fa...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take a peek at the data\n",
    "dftxt.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenizing, Stemming, and Stop Words\n",
    "#### Sample text:\n",
    ">*I changed my email address to jzhang28@stanford.edu.*\n",
    "\n",
    "#### Tokens:\n",
    ">*i, changed, my, email, address, to, jzhang28@stanford.edu*\n",
    "\n",
    "#### Stems:\n",
    ">*i, <span style=\"color:red\">chang</span>, my, email, address, to, jzhang28@stanford.edu*\n",
    "\n",
    "#### Stop words:\n",
    ">*<strike style=\"color:red\">i</strike>, chang, <strike style=\"color:red\">my</strike>, email, address, <strike style=\"color:red\">to</strike>, jzhang28@stanford.edu*\n",
    "\n",
    "#### N-grams (bi-grams):\n",
    ">*chang-email, email-address, address-jzhang28@stanford.edu*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import tokenize, stem, corpus\n",
    "\n",
    "#Create regular expressions in order to tokenize string\n",
    "email_regex_str = '\\w[a-z0-9._-]+\\w@[a-z0-9._-]+\\w' #email pattern \n",
    "currency_regex_str = '[$£€¥][\\d,\\.]+' #currency pattern\n",
    "ssn_regex_str = '\\d{3}-\\d{2}-\\d{4}' #ssn pattern\n",
    "word_regex_str = '\\w+' #any word\n",
    "token_regex = '|'.join([email_regex_str, currency_regex_str, ssn_regex_str, word_regex_str]) #set precedence\n",
    "\n",
    "#priming the regular expressions for faster performance\n",
    "email_regex = re.compile(email_regex_str)\n",
    "currency_regex = re.compile(currency_regex_str)\n",
    "ssn_regex = re.compile(ssn_regex_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenize function\n",
    "def get_tokens(text):\n",
    "    text = text.lower()\n",
    "    return tokenize.regexp_tokenize(text, token_regex , gaps=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stemmer function\n",
    "#Iterate through tokens and stem the words if applicable\n",
    "#Transformed currency amount to an entity tag\n",
    "#Transformed SSN's to an entity tag\n",
    "stemmer = stem.snowball.EnglishStemmer()\n",
    "def get_stems(tokens):\n",
    "    stems = []\n",
    "    for t in tokens:\n",
    "        if email_regex.match(t):\n",
    "            stems.append(t)\n",
    "        elif currency_regex.match(t):\n",
    "            stems.append('_currency_amt_')\n",
    "        elif ssn_regex.match(t):\n",
    "            stems.append('_ssn_')\n",
    "        elif t.isalpha() and len(t) > 1:\n",
    "            stems.append(stemmer.stem(t)) \n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stop word removal function\n",
    "stopwords = set(corpus.stopwords.words('english'))\n",
    "addl_stopwords = set(['pleas','thank','edu'])\n",
    "custom_stopwords = stopwords | addl_stopwords\n",
    "def remove_stop_words(tokens):\n",
    "    return [t for t in tokens if t not in custom_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Apply functions to dataset\n",
    "dftxt['tokens'] = dftxt['text'].map(get_tokens)\n",
    "dftxt['stems'] = dftxt['tokens'].map(get_stems)\n",
    "dftxt['cleaned_stems'] = dftxt['stems'].map(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text:\n",
      "ERS Supplier. Request a change to information for an existing payee/supplier.  Identify payee/supplier as displayed in Supplier Query and Request module. Payee/Supplier Name: SMITH, JOHN Payee/Supplier Number: \t123456  Use this form to designate these types of changes. Please indicate the item(s) that are changing.  __ Contact (name, phone, email, fax)   Provide detailed descriptions of the changes (also indicate deletion, inactivation, or changes to existing):  Please update the email address to: johnsmith@gmail.com\n",
      "\n",
      "\n",
      "Tokens:\n",
      "['ers', 'supplier', 'request', 'a', 'change', 'to', 'information', 'for', 'an', 'existing', 'payee', 'supplier', 'identify', 'payee', 'supplier', 'as', 'displayed', 'in', 'supplier', 'query', 'and', 'request', 'module', 'payee', 'supplier', 'name', 'smith', 'john', 'payee', 'supplier', 'number', '123456', 'use', 'this', 'form', 'to', 'designate', 'these', 'types', 'of', 'changes', 'please', 'indicate', 'the', 'item', 's', 'that', 'are', 'changing', '__', 'contact', 'name', 'phone', 'email', 'fax', 'provide', 'detailed', 'descriptions', 'of', 'the', 'changes', 'also', 'indicate', 'deletion', 'inactivation', 'or', 'changes', 'to', 'existing', 'please', 'update', 'the', 'email', 'address', 'to', 'johnsmith@gmail.com']\n",
      "\n",
      "Stems:\n",
      "['er', 'supplier', 'request', 'chang', 'to', 'inform', 'for', 'an', 'exist', 'paye', 'supplier', 'identifi', 'paye', 'supplier', 'as', 'display', 'in', 'supplier', 'queri', 'and', 'request', 'modul', 'paye', 'supplier', 'name', 'smith', 'john', 'paye', 'supplier', 'number', 'use', 'this', 'form', 'to', 'design', 'these', 'type', 'of', 'chang', 'pleas', 'indic', 'the', 'item', 'that', 'are', 'chang', 'contact', 'name', 'phone', 'email', 'fax', 'provid', 'detail', 'descript', 'of', 'the', 'chang', 'also', 'indic', 'delet', 'inactiv', 'or', 'chang', 'to', 'exist', 'pleas', 'updat', 'the', 'email', 'address', 'to', 'johnsmith@gmail.com']\n",
      "\n",
      "Cleaned Stems:\n",
      "['er', 'supplier', 'request', 'chang', 'inform', 'exist', 'paye', 'supplier', 'identifi', 'paye', 'supplier', 'display', 'supplier', 'queri', 'request', 'modul', 'paye', 'supplier', 'name', 'smith', 'john', 'paye', 'supplier', 'number', 'use', 'form', 'design', 'type', 'chang', 'indic', 'item', 'chang', 'contact', 'name', 'phone', 'email', 'fax', 'provid', 'detail', 'descript', 'chang', 'also', 'indic', 'delet', 'inactiv', 'chang', 'exist', 'updat', 'email', 'address', 'johnsmith@gmail.com']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Check text processing on a record\n",
    "ind=0\n",
    "print('''\n",
    "Text:\\n%s\\n\\n\n",
    "Tokens:\\n%s\\n\n",
    "Stems:\\n%s\\n\n",
    "Cleaned Stems:\\n%s\\n\n",
    "''' % (dftxt.iloc[ind]['text'], dftxt.iloc[ind]['tokens'],dftxt.iloc[ind]['stems'],dftxt.iloc[ind]['cleaned_stems']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>3. Document Term Matrix</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/dtm.JPG\" style=\"float: left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align*}\n",
    "&Term\\ Count\\ = \\text{Count of number of times term t occurs in document D}\\\\\n",
    "&Term\\ Frequency\\ (TF) = \\frac{Term\\ Count}{Total\\ Term\\ Count\\ in\\ document\\ D}\\\\\n",
    "&TF.IDF = TF \\times Inverse\\ Document\\ Frequency\\\\\n",
    "\\end{align*}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF.IDF\n",
    "\n",
    "* If a term appears in almost every document, it does not provide any information\n",
    "* Conversely, if a term appears in a subset of documents, it is insight we want to extract\n",
    "* Therefore, we want to weigh the latter higher than the former\n",
    "* IDF is merely this weight\n",
    "\n",
    "$IDF(t) = log{(1+\\frac{Total\\ number\\ of\\ documents}{Number\\ of\\ documents\\ where\\ term\\ t\\ appears})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Generate Term Count and TFIDF matricies\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "\n",
    "#sklearn vectorizer functions take text data in the form of a string\n",
    "# and allows us to defind a function to process that string.\n",
    "# Define that function below\n",
    "def process_text(text):\n",
    "    tokens = get_tokens(text)\n",
    "    stems = get_stems(tokens)\n",
    "    cleaned_stems = remove_stop_words(stems)\n",
    "    return cleaned_stems\n",
    "\n",
    "#Create Term Count matrix\n",
    "#max_df: Exclude terms that appear in over 95% of the documents\n",
    "#min_df: Exclude terms that appear in under 1% of the documents\n",
    "#ngram_range: Single words and bi-grams\n",
    "cnt_vect = CountVectorizer(tokenizer=process_text, max_df=0.95, min_df=0.01, ngram_range=(1,2))\n",
    "cnt_matrix = cnt_vect.fit_transform(dftxt['text'])\n",
    "\n",
    "#Extract list of terms to be used as reference later\n",
    "terms = cnt_vect.get_feature_names()\n",
    "\n",
    "#Using the Term Count matrix, we can create the TFIDF matrix\n",
    "tfidf_matrix = TfidfTransformer().fit_transform(cnt_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term count matrix rows and columns: (7628, 794)\n",
      "TFIDF matrix rows and columns: (7628, 794)\n",
      "Number of terms: 794\n"
     ]
    }
   ],
   "source": [
    "print(\"Term count matrix rows and columns:\", cnt_matrix.shape)\n",
    "print(\"TFIDF matrix rows and columns:\", tfidf_matrix.shape)\n",
    "print(\"Number of terms:\", len(terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Topic Modeling\n",
    "<img src=\"img/topics_matrix.jpg\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Topic modeling with Non-negative Matrix Factorization\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "#Specify number of topics to find\n",
    "k_topics = 10\n",
    "\n",
    "#Create NMF model object\n",
    "nmf = NMF(init='nndsvd', n_components=k_topics, random_state=1)\n",
    "\n",
    "#Run model and output the TFIDF document topic matrix \n",
    "nmf_topic_matrix = nmf.fit_transform(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7628, 10)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check matrix rows and columns\n",
    "nmf_topic_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Define convenience function to print out top words contained topic term matrix\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_): #model.components_ attribute contains the topic term matrix\n",
    "        words = [feature_names[i].replace(' ','-') for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        print(\"Topic #%d: %s\" % (topic_idx, \", \".join(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: chang, paye, paye-supplier, supplier, name, author-agent, author, agent\n",
      "Topic #1: stanford, univers, stanford-univers, payment, inform, messag, univers-procur, paye\n",
      "Topic #2: supplier-request, request, supplier, request-supplier, cancel, info, status, request-request\n",
      "Topic #3: joni, fakename@stanford.edu, png, joni-fakenam, fakenam, supplier-set, procur-financi, manag-servic\n",
      "Topic #4: supplier-enabl, enabl-team, team, enabl, email, sent, supplier, helpsu\n",
      "Topic #5: reactiv, iprocur, inc, supplier, er, llc, enabl, paye\n",
      "Topic #6: ssn, need-ssn, provid, need, ssn-supplier, call, _ssn_, setup\n",
      "Topic #7: portal, portal-request, resend, secur-portal, secur, send, request, resent\n",
      "Topic #8: address, inc, site, updat, po, address-updat, need, activ\n",
      "Topic #9: req, supplier-req, supplier, iprocur, resent, status, set, see\n"
     ]
    }
   ],
   "source": [
    "#NMF topics and terms\n",
    "print_top_words(nmf, terms, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text of the first document:\n",
      " ERS Supplier. Request a change to information for an existing payee/supplier.  Identify payee/supplier as displayed in Supplier Query and Request module. Payee/Supplier Name: SMITH, JOHN Payee/Supplier Number: \t123456  Use this form to designate these types of changes. Please indicate the item(s) that are changing.  __ Contact (name, phone, email, fax)   Provide detailed descriptions of the changes (also indicate deletion, inactivation, or changes to existing):  Please update the email address to: johnsmith@gmail.com\n",
      "\n",
      "NMF matrix first row:\n",
      "f0: 0.1217\n",
      "f1: 0.0\n",
      "f2: 0.021\n",
      "f3: 0.0\n",
      "f4: 0.0\n",
      "f5: 0.0009\n",
      "f6: 0.0\n",
      "f7: 0.0\n",
      "f8: 0.0172\n",
      "f9: 0.0036\n"
     ]
    }
   ],
   "source": [
    "print(\"Text of the first document:\\n\", dftxt.iloc[0]['text'])\n",
    "print()\n",
    "print(\"NMF matrix first row:\")\n",
    "for idx, value in enumerate(nmf_topic_matrix[0]):\n",
    "    print(\"f%s: %s\" % (idx,round(value,4)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Topic modeling with Latent Dirichlet Allocation\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "#Create LDA model object\n",
    "lda = LatentDirichletAllocation(n_topics=k_topics, max_iter=5, learning_method='online', learning_offset=50., random_state=1)\n",
    "\n",
    "#Run model on term count matrix\n",
    "#LDA requires integer values\n",
    "lda_topic_matrix = lda.fit_transform(cnt_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7628, 10)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check matrix rows and columns\n",
    "lda_topic_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: stanford, subject, sent, payment, email, univers, help123@stanford.edu, inform\n",
      "Topic #1: address, updat, bank, supplier, email, inc, wire, po\n",
      "Topic #2: supplier, email, enabl, secur, request, sent, inform, team\n",
      "Topic #3: need, paye, reimburs, set, complet, check, status, transact\n",
      "Topic #4: request, supplier, supplier-request, portal, site, reactiv, inc, activ\n",
      "Topic #5: supplier, paye, paye-supplier, chang, name, email, request, inform\n",
      "Topic #6: supplier, ssn, req, supplier-req, call, er, ssn-supplier, su\n",
      "Topic #7: email, messag, financi, ani, support, financi-support, receiv, center\n",
      "Topic #8: stanford, univers, stanford-univers, payment, inform, request, supplier, paye\n",
      "Topic #9: request, joni, supplier, number, fakename@stanford.edu, alto, palo, palo-alto\n"
     ]
    }
   ],
   "source": [
    "#LDA topic and terms\n",
    "print_top_words(lda, terms, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA matrix first row:\n",
      "f0: 0.0011\n",
      "f1: 0.0011\n",
      "f2: 0.0011\n",
      "f3: 0.0011\n",
      "f4: 0.0011\n",
      "f5: 0.9776\n",
      "f6: 0.0138\n",
      "f7: 0.0011\n",
      "f8: 0.0011\n",
      "f9: 0.0011\n"
     ]
    }
   ],
   "source": [
    "print(\"LDA matrix first row:\")\n",
    "for idx, value in enumerate(lda_topic_matrix[0]):\n",
    "    print(\"f%s: %s\" % (idx,round(value,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most likely topic for the first document:\n",
      "NMF topic: [0]\n",
      "LDA topic: [5]\n"
     ]
    }
   ],
   "source": [
    "print('Most likely topic for the first document:')\n",
    "print('NMF topic: %s\\nLDA topic: %s' % (nmf_topic_matrix[0].argsort()[:-2:-1],lda_topic_matrix[0].argsort()[:-2:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Predictive Text Classifier\n",
    "<img src=\"img/ml.jpg\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cat B      0.555585\n",
       "Cat A    0.444415\n",
       "Name: Operational Categorization Tier 3, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use this feature as the target variable y\n",
    "dftxt['Operational Categorization Tier 3'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prep data for predictive model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse\n",
    "\n",
    "#Transform target variable into an array of binary outcomes\n",
    "target_values = np.where(dftxt['Operational Categorization Tier 3'] == 'Cat B', 1, 0)\n",
    "\n",
    "#Combine target values with tfidf_matrix data\n",
    "pred_data_set = sparse.hstack([tfidf_matrix, target_values[:,None]]).toarray()\n",
    "\n",
    "#Randomly split data into training and testing sets\n",
    "train, test = train_test_split(pred_data_set)\n",
    "\n",
    "#Separate features from target variable for train and test datasets\n",
    "y_train = [t[0] for t in train[:,-1:]]\n",
    "y_test = [t[0] for t in test[:,-1:]]\n",
    "X_train = train[:,:train.shape[1]-1]\n",
    "X_test = test[:,:test.shape[1]-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set features: (5721, 794)\n",
      "Training set target variable: 5721\n",
      "\n",
      "Test set features: (1907, 794)\n",
      "Test set target variable: 1907\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set features:\", X_train.shape)\n",
    "print(\"Training set target variable:\", len(y_train))\n",
    "print()\n",
    "print(\"Test set features:\", X_test.shape)\n",
    "print(\"Test set target variable:\", len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Establish baseline model\n",
    "#The simplest model is to predict that everything is Cat B, i.e. 1\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#Generate my baseline predictions of all 1's\n",
    "baseline_pred_values = [1] * len(y_test)\n",
    "\n",
    "#Score my prediction agains the test set y values\n",
    "auc_baseline = roc_auc_score(y_test, baseline_pred_values)\n",
    "auc_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73245618955729574"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run Naive-Bayes classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Generate the model using the training set\n",
    "nb_model = MultinomialNB().fit(X_train, y_train)\n",
    "\n",
    "#Generate prediction from the model by feeding in the test set X values\n",
    "nb_pred_values = nb_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "#Score the model predictions against the test set y values\n",
    "auc_nb = roc_auc_score(y_test, nb_pred_values)\n",
    "auc_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Resources\n",
    "\n",
    "<a href=\"http://scikit-learn.org\">scikit-learn</a>: Contains documentation on how to use the scikit python package along with information on the models used<br/>\n",
    "<a href=\"https://www.youtube.com/watch?v=UzxYlbK2c7E&list=PLA89DCFA6ADACE599\">Machine learning lecutures by Prof. Andrew Ng</a>: Lectures recorded at Stanford. A bit math heavy but provides fundamental concepts behind machine learning<br/>\n",
    "<a href=\"http://www.quuxlabs.com/blog/2010/09/matrix-factorization-a-simple-tutorial-and-implementation-in-python/\">Matrix factorization</a>: math and intuition behind matrix factorization, the basis of the NMF model<br/>\n",
    "<a href=\"http://videolectures.net/mlss09uk_blei_tm/\">Topic modeling lecture</a>: Has good explanation on how LDA works<br/>\n",
    "<a href=\"https://www.youtube.com/watch?v=TpgiFIGXcT4\">Bayesian statistics video</a>: Video lecture from PyCon 2016.<br/>\n",
    "<a href=\"http://brandonrose.org/clustering\">More examples of document clustering using Python</a><br />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
